<html>

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Fei Zhao</title>
    <meta content="Fei Zhao, https://1429904852.github.io/" name="keywords">
    <link rel="stylesheet" type="text/css" href="resources/css/mystyle.css">
    <link rel="stylesheet" type="text/css" href="resources/css/font.css">
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-164510176-1"></script>
    <script async defer src="https://buttons.github.io/buttons.js"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() {
            dataLayer.push(arguments);
        }
        gtag('js', new Date());
        gtag('config', 'UA-164510176-1');
    </script>
</head>


<body>
    <!--photo and basic information-->

    <!-- <div style="margin-bottom: 1em; border: 1px solid #ddd; background-color: #fff; padding: 1em; height: 130px;">
        <div style="margin: 0px auto; width: 100%;">
            <img title="feizhao" style="float: left; padding-left: .01em; height: 130px;"
                 src="./resources/images/me/me2.jpg">
            <div style="padding-left: 10em; vertical-align: top; height: 120px;">
                <span style="line-height: 150%; font-size: 20pt;">Fei Zhao (赵飞)</span><br>
                <span><a
                    href="https://ai.nju.edu.cn/main.htm">School of Artificial Intelligence</a>, <a
                        href="https://www.nju.edu.cn/main.htm">Nanjing University
                            (NJU)</a></span><br>
                <span><a
                    href="https://keysoftlab.nju.edu.cn/main.htm">State Key Laboratory of Novel Software Technology</a></span><br>
                <span><strong>Address</strong>: 163 Xianlin Avenue, Qixia District, Nanjing, China</span><br>
                <span><strong>Email</strong>: zhaof [at] smail.nju.edu.cn </span> <br>
            </div>
        </div>
    </div> -->

    <iframe class="info" src="resources/html/info.html"
        style="border: 1px solid #ddd; margin-bottom: 1em; padding: 1em; background-color: #fff;"></iframe>
    
    <!--biography-->
    <div style="clear: both;">
        <div class="section">
            <h2>About Me (<a href="https://github.com/1429904852">[GitHub]</a>
                <a href="https://scholar.google.com/citations?user=V01xzWQAAAAJ&hl=zh-CN">[Google Scholar]</a>)
            </h2>
            <div class="paper">
                I am currently a final-year PhD candidate at <a
                href="https://ai.nju.edu.cn/main.htm">School of Artificial Intelligence</a>, <a
                href="https://www.nju.edu.cn/main.htm">Nanjing University
                    (NJU)</a>, and a member of <a
                href="http://nlp.nju.edu.cn/homepage/">NJU-NLP</a> Research Group. My recent works are mainly on in-context learning, multi-modal information extraction, multi-modal large language models, and NLP applications.
            </div>
        </div>
    </div>

    <!-- <div style="clear: both;">
        <div class="section">
            <h2>Research Interest</a>
            </h2>
            <div class="paper">
                My recent works are mainly on in-context learning and NLP applications, multi-modal information extraction, and multi-modal large language models.
                <!-- My recent works are mainly on: -->
                <!-- <ul>
                    <li>In-context learning and NLP Applications</li>
                    <li>Multi-modal Information Extraction</li>
                    <li>Multi-modal Large Language Models</li>
                </ul> -->
            <!-- </div> -->
        <!-- </div> -->
    <!-- </div> -->

    <!--News-->
    <div style="clear: both;">
        <div class="section">
            <h2 id="news">Latest News</h2>
            <div class="paper">
                <ul>
                    <li>
                        2025-08-20: One paper on Chinese Multi-image Benchmark is accepted by Findings of EMNLP 2025.
                    </li>
                    <li>
                        2025-02-22: One paper on Efficient Multimodal Large Language Models is accepted by ICLR 2025.
                    </li>
                    <li>
                        2024-09-20: One paper on Multimodal Hallucination Mitigation is accepted by EMNLP 2024.
                    </li>
                    <li>
                        2024-05-24: Release a paper on <a href="https://arxiv.org/abs/2405.14129">Multi-modal Large Language Models</a> to Arxiv.
                    </li>
                    <li>
                        2024-05-16: Two papers are accepted by Findings of ACL 2024.
                    </li>
                    <li>
                        2024-02-16: Release a paper on <a href="https://arxiv.org/pdf/2402.09801.pdf">Multimodal Hallucination Mitigation</a> to Arxiv.
                    </li>
                    <li>
                        2023-10-08: One paper on multimodal ABSA is accepted by EMNLP 2023.
                    </li>
                    <li>
                        2023-10-03: Release a paper on <a href="https://arxiv.org/pdf/2310.00385.pdf">in-context learning</a> to Arxiv.
                    </li>
                    <li>
                        2023-07-26: One paper on multimodal entity linking is accepted by ACM Multimedia 2023.
                    </li>
                    <li>
                        2023-05-11: One paper is accepted by TASLP 2023.
                    </li>
                    <li>
                        2023-05-02: One paper is accepted by Findings of ACL 2023.
                    </li>
                    <li>
                        2022-10-06: One paper on <a href="https://aclanthology.org/2022.findings-emnlp.177.pdf">few-shot aspect category detection</a> is accepted by Findings of EMNLP 2022.
                    </li>
                    <li>
                        2022-08-17: One paper on multimodal ABSA is accepted by COLING 2022.
                    </li>
                    <li>
                        2022-06-30: One paper on <a href="./resources/paper/ACMMM2022.pdf">multimodal NER</a> is accepted by ACM Multimedia 2022.
                    </li>
                    <li>
                        2020-10-01: One paper on ABSA is accepted by COLING 2020.
                    </li>
                    <li>
                        2020-09-14: One paper on aspect triplet extraction is accepted by Findings of EMNLP 2020.
                    </li>
                    <li>
                        2019-11-11: One paper on <a href="./resources/paper/AAAI2020.pdf">TOWE</a> is accepted by AAAI 2020.
                    </li>
                </ul>
                <div class="spanner"></div>
            </div>
        </div>
    </div>

    <div style="clear: both;">
        <div class="section">
            <h2 id="confpapers">Pre-prints </h2>
            <div class="paper"><img class="paper" src="./resources/paper_icon/AlignGPT.png"
                title="AlignGPT: Multi-modal Large Language Models with Adaptive Alignment Capability">
                <div><strong>AlignGPT: Multi-modal Large Language Models with Adaptive Alignment Capability</strong><br>
                    <strong>Fei Zhao</strong>*, Taotian Pang*, Chunhui Li, Zhen Wu, Junjie Guo, Shangyu Xing, and Xinyu Dai<br>
                    Under review <br>
                    <!-- Under review <br> -->
                    <a href="https://aligngpt-vl.github.io/">[Project page]</a>
                    <a href="https://arxiv.org/abs/2405.14129">[Paper]</a>
                    <a href="https://github.com/AlignGPT-VL/AlignGPT">[Code]</a>
                    <a href="https://huggingface.co/nlpzhaof">[Model]</a>
                    <a href="http://47.116.173.89:7870/">[Demo]</a>
                    <!-- <a href="">[BibTex]</a> -->
                    <br>
                    <alert> AlignGPT generates different levels of alignment capabilities in the pre-training
                        stage, and then adaptively combines these alignment capabilities in the instruction-tuning stage
                        to meet the alignment needs of different instructions.</alert>
                </div>
            </div>
            <div class="paper"><img class="paper" src="./resources/paper_icon/Under_review.png"
                title="Dynamic Demonstrations Controller for In-Context Learning">
                <div><strong>Dynamic Demonstrations Controller for In-Context Learning</strong><br>
                    <strong>Fei Zhao</strong>, Taotian Pang, Zhen Wu, Zheng Ma, Shujian Huang, Xinyu Dai<br>
                    Under review <br>
                    <a href="https://arxiv.org/pdf/2310.00385.pdf">[Paper]</a>
                    <!-- <a href="">[Slides]</a> -->
                    <a href="https://github.com/TJTP/D2Controller">[Code]</a>
                    <a href="">[中文解读]</a>
                    <a href="">[BibTex]</a>
                    <br>
                    <alert> We propose a method named D2Controller, which not only boosts ICL performance but also
                        saves time and space during inference of the LLMs. </alert>
                </div>
                <div class="spanner"></div>
            </div>
        </div>
    </div>

    <!--selected publications-->
    <div style="clear: both;">
        <div class="section">
            <h2 id="confpapers">Selected Publications (<a href="./resources/html/publication.html">[Full List]</a>)</h2>
            (* indicates equal contribution)

            <div class="paper"><img class="paper" src="./resources/paper_icon/RealBench.png"
                title="RealBench: A Chinese Multi-image Understanding Benchmark Close to Real-world Scenarios">
                <div><strong>RealBench: A Chinese Multi-image Understanding Benchmark Close to Real-world Scenarios</strong><br>
                    <strong>Fei Zhao</strong>, Chengqiang Lu, Yufan Shen, Qimeng Wang, Yicheng Qian, Haoxin Zhang, Yan Gao, Yi Wu, Yao Hu, Zhen Wu, Shangyu Xing, Xinyu Dai<br>
                    EMNLP findings, 2025 <br>
                    <a href="">[Paper]</a>
                    <!-- <a href="">[Slides]</a> -->
                    <a href="">[Code]</a>
                    <a href="">[中文解读]</a>
                    <a href="">[BibTex]</a>
                    <br>
                    <alert> We introduce RealBench, the first Chinese multi-image benchmark built from public user-generated content. </alert>
                </div>
            </div>
            
            <div class="paper"><img class="paper" src="./resources/paper_icon/EFUF.png"
                title="EFUF: Efficient Fine-grained Unlearning Framework for Mitigating Hallucinations in Multimodal Large Language Models">
                <div><strong>EFUF: Efficient Fine-grained Unlearning Framework for Mitigating Hallucinations in Multimodal Large Language Models</strong><br>
                    Shangyu Xing, <strong>Fei Zhao</strong>, Zhen Wu, Tuo An, Weihao Chen, Chunhui Li, Jianbing Zhang, Xinyu Dai<br>
                    EMNLP, 2024 <br>
                    <a href="https://arxiv.org/pdf/2402.09801.pdf">[Paper]</a>
                    <!-- <a href="">[Slides]</a> -->
                    <a href="https://github.com/starreeze/efuf">[Code]</a>
                    <a href="">[中文解读]</a>
                    <a href="">[BibTex]</a>
                    <br>
                    <alert> We propose an efficient fine-grained unlearning framework EFUF, which can obtain positive and negative examples separately in a
                        cost-effective manner. </alert>
                </div>
            </div>

            <div class="paper"><img class="paper" src="./resources/paper_icon/EMNLP2023.jpg"
                title="M2DF: Multi-grained Multi-curriculum Denoising Framework for Multimodal Aspect-based Sentiment Analysis">
                <div><strong>M2DF: Multi-grained Multi-curriculum Denoising Framework for Multimodal Aspect-based Sentiment Analysis</strong><br>
                    <strong>Fei Zhao*</strong>, Chunhui Li*, Zhen Wu, Yawen Ouyang, Jianbing Zhang and Xinyu Dai<br>
                    EMNLP, 2023 <br>
                    <a href="https://aclanthology.org/2023.emnlp-main.561.pdf">[Paper]</a>
                    <!-- <a href="">[Slides]</a> -->
                    <a href="https://github.com/grandchicken/M2DF">[Code]</a>
                    <a href="https://mp.weixin.qq.com/s/2y85G3wxI4PRTteuUax-Yw">[中文解读]</a>
                    <a href="">[BibTex]</a>
                    <br>
                    <alert>  In this work, we focus on whether the negative impact of noisy images can be reduced without filtering the data. </alert>
                </div>
                <div class="spanner"></div>
            </div>
            
            <div class="paper"><img class="paper" src="./resources/paper_icon/ACMMM2023.jpg"
                title="DRIN: Dynamic Relation Interactive Network for Multimodal Entity Linking">
                <div><strong>DRIN: Dynamic Relation Interactive Network for Multimodal Entity Linking</strong><br>
                    Shangyu Xing*, <strong>Fei Zhao*</strong>, Zhen Wu, Chunhui Li, Jianbing Zhang and Xinyu Dai<br>
                    ACM MM, 2023 <br>
                    <a href="./resources/paper/ACMMM2023.pdf">[Paper]</a>
                    <!-- <a href="">[Slides]</a> -->
                    <a href="https://github.com/starreeze/drin">[Code]</a>
                    <a href="https://mp.weixin.qq.com/s/6-xm53c2rMSu9q1qUV3cag">[中文解读]</a>
                    <a href="">[BibTex]</a>
                    <br>
                    <alert> DRIN explicitly models four different types of alignment between a mention and entity and builds a dynamic Graph
Convolutional Network (GCN).</alert>
                </div>
                <div class="spanner"></div>
            </div>

            <div class="paper"><img class="paper" src="./resources/paper_icon/EMNLP2022_LDF.png"
                title="Label-Driven Denoising Framework for Multi-Label Few-Shot
                Aspect Category Detection">
                <div><strong>Label-Driven Denoising Framework for Multi-Label Few-Shot
                    Aspect Category Detection</strong><br>
                    <strong>Fei Zhao*</strong>, Yuchen Shen*, Zhen Wu, Xinyu Dai<br>
                    EMNLP findings, 2022 <br>
                    <a href="https://aclanthology.org/2022.findings-emnlp.177.pdf">[Paper]</a>
                    <a href="./resources/slides/EMNLP2022.pdf">[Slides]</a>
                    <a href="https://github.com/1429904852/LDF">[Code]</a>
                    <a href="https://mp.weixin.qq.com/s/YcTJK2wmNCeVicuO0fX-Ag">[中文解读]</a>
                    <a href="./resources/bibtex/EMNLP2022_Findings_LDF.txt">[BibTex]</a>
                    <br>
                    <alert>We propose a novel Label-Driven Denoising Framework (LDF) to alleviate the noise
                        problems for the FS-ACD task.</alert>
                </div>
                <div class="spanner"></div>
            </div>
            
           <div class="paper"><img class="paper" src="./resources/paper_icon/ACMMM2022_RGCN.png"
                title="Learning from Different text-image Pairs: A Relation-enhanced
                Graph Convolutional Network for Multimodal NER">
                <div><strong>Learning from Different text-image Pairs: A Relation-enhanced
                Graph Convolutional Network for Multimodal NER</strong><br>
                    <strong>Fei Zhao</strong>, Chunhui Li, Zhen Wu, Shangyu Xing, Xinyu Dai<br>
                    ACM MM, 2022 <br>
                    <a href="./resources/paper/ACMMM2022.pdf">[Paper]</a>
                    <a href="./resources/slides/ACMMM2022.pdf">[Slides]</a>
                    <a href="https://github.com/1429904852/R-GCN">[Code]</a>
                    <a href="https://mp.weixin.qq.com/s/cDAXxLrDIVjWofLFaam_KA">[中文解读]</a>
                    <a href="./resources/bibtex/ACMMM2022_R_GCN.txt">[BibTex]</a>
                    <br>
                    <alert>We propose leveraging the external matching relations between different (text, image) pairs to improve the performance on the MNER task.</alert>
                </div>
                <div class="spanner"></div>
            </div>
            
            <div class="paper"><img class="paper" src="./resources/paper_icon/AAAI2020_LOTN.png"
                title="Latent Opinions Transfer Network for Target-Oriented Opinion Words Extraction">
                <div><strong>Latent Opinions Transfer Network for Target-Oriented Opinion Words Extraction</strong><br>
                    Zhen Wu*, <strong>Fei Zhao*</strong>, Xin-Yu Dai, Shujian Huang, Jiajun Chen<br>
                    AAAI, 2020 <alert>(oral presentation)</alert><br>
                    <a href="./resources/paper/AAAI2020.pdf">[Paper]</a>
                    <a href="./resources/slides/AAAI2020.pdf">[Slides]</a>
                    <!-- <a href="https://github.com/1429904852/LOTN">[Code]</a><img src="https://img.shields.io/github/stars/1429904852/LOTN?style=social"/> -->
                    <a href="https://github.com/1429904852/LOTN">[Code]</a>
                    <a href="https://mp.weixin.qq.com/s/rO_dGBt_x6uIXLsjka5kKA">[中文解读]</a>
                    <a href="./resources/bibtex/AAAI2020_LOTN.txt">[BibTex]</a>
                    <br>
                    <alert>We transter latent opinion knowledge from resource-rich review sentiment classification datasets to low-resource TOWE task.</alert>
                </div>
                <div class="spanner"></div>
            </div>
        </div>
    </div>
    
    <div style="clear: both;">
        <div class="section">
            <h2 id="Journal Papers">Journal Papers</h2>
        
           <div class="paper"><img class="paper" src="./resources/paper_icon/LCN.png"
                title="Label-correction Capsule Network for Hierarchical
                Text Classification">
                <div><strong>Label-correction Capsule Network for Hierarchical
                    Text Classification</strong><br>
                    <strong>Fei Zhao</strong>, Zhen Wu, Liang He, Xinyu Dai<br>
                    IEEE Transactions on Audio, Speech and Language Processing(TASLP), 2023 <br>
                    <a href="./resources/paper/TASLP2023.pdf">[Paper]</a>
                    <a href="">[Slides]</a>
                    <a href="https://github.com/1429904852/LCN_Capsule">[Code]</a>
                    <a href="">[中文解读]</a>
                    <a href="./resources/bibtex/TASLP2023_LCN.txt">[BibTex]</a>
                    <br>
                    <alert>We design two novel approaches to weaken the impact
                        of incorrect parent-level labels on the child-level classification.</alert>
                </div>
                <div class="spanner"></div>
            </div>
        </div>
    </div>

    <!--Experience-->
    <div style="clear: both;">
        <div class="section">
            <h2 id="experience">Experience</h2>
            <div class="paper">
                <ul>
                    <li>
                        Jun. 2024 - Nov. 2024, research intern at Xiaohongshu.
                    </li>
                    <li>
                        Apr. 2020 - Nov. 2020, research intern at Tencent WeChat Group.
                    </li>
                </ul>
                <div class="spanner"></div>
            </div>
        </div>
    </div>
    
    <!--Honor-->
    <div style="clear: both;">
        <div class="section">
            <h2 id="confpapers">Honor</h2>
            <div class="paper">
                <ul>
                    <li>
                        National Scholarship, 2018.
                    </li>
                    <li>
                        Outstanding Graduate, 2018.
                    </li>
                    <li>
                        Outstanding Undergraduation Thesis, 2018.
                    </li>
                    <li>
                        Huawei Scholarship, 2020.
                    </li>
                    <!-- <li>
                        Outstanding Student, 2020.
                    </li> -->
                    <li>
                        Huawei Scholarship, 2022.
                    </li>
                    <li>
                        First-Class Yingcai Scholarship, 2023.
                    </li>
                </ul>
                <div class="spanner"></div>
            </div>
        </div>
    </div>

    <div style="clear: both;">
        <div class="section">
            <h2 id="confpapers">Teaching Assistant</h2>
            <div class="paper">
                <ul>
                    <li>
                        Natural Language Processing (For graduate students, Fall, 2021)
                    </li>
                    <li>
                        Natural Language Processing (For undergraduate students, Spring, 2022)
                    </li>
                </ul>
                <div class="spanner"></div>
            </div>
        </div>
    </div>

    <!--Review Service-->
    <div style="clear: both;">
        <div class="section">
            <h2>Review Services</h2>
            <div class="paper">
                <strong>Program Committee Member/Conference Reviewer</strong><br>
                International Conference on Computer Vision (ICCV), 2025<br>
                Forty-Second International Conference on Machine Learning (ICML), 2025<br>
                International World Wide Web Conference (WWW), 2025<br>
                The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025<br>
                International Conference on Learning Representations (ICLR), 2025<br>
                Neural Information Processing Systems (NIPS), 2024<br>
                Conference on Language Modeling (COLM), 2024<br>
                ACL Rolling Review 2021, 2022, 2023, 2024<br>
                ACM Multimedia (ACM MM), 2023<br>
                Empirical Methods in Natural Language Processing (EMNLP), 2022, 2023<br>
                AAAI Conference on Artificial Intelligence (AAAI), 2021, 2022, 2023, 2024<br>
                Annual Meeting of the Association for Computational Linguistics (ACL), 2021, 2023<br>
                North American Chapter of the Association for Computational Linguistics (NAACL), 2022<br>
                International Workshop on Natural Language Processing for Social Media (SocialNLP), 2021<br>
            </div>
        </div>
    </div>
    <div style='width:800px;height:100px;margin:0 auto'>
        <!-- <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=a&t=tt&d=Uhn1Ki07Wd6fYaEsf066kGFmDzTsBdfs7JhrRNbePDU'></script> -->
        <!-- <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=300&t=tt&d=Uhn1Ki07Wd6fYaEsf066kGFmDzTsBdfs7JhrRNbePDU'></script> -->
        <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=a&t=tt&d=Uhn1Ki07Wd6fYaEsf066kGFmDzTsBdfs7JhrRNbePDU'></script>
        <!-- <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=Uhn1Ki07Wd6fYaEsf066kGFmDzTsBdfs7JhrRNbePDU&cl=ffffff&w=a"></script> -->
    </div>
</body>

</html>
