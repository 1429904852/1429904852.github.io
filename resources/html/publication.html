<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">

<html>

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <!-- <meta content="text/html; charset=windows-1252" http-equiv="Content-Type"> -->
    <link rel="shortcut icon" href="images/1.jpg">
    <link rel="stylesheet" type="text/css" href="static/css/bootstrap.min.css">
    <link rel="stylesheet" type="text/css" href="static/css/main.css" media="screen,projection">
    <link rel="stylesheet" type="text/css" href="static/css/custom.css" media="screen,projection">
    <link rel="stylesheet" type="text/css" href="style2.css"/>
    <title>Fei Zhao</title>
</head>


<body>
<style>a {
    text-decoration: none;
}</style>
<div id="header">
    <table style="text-align: left; width: 80%;" cellspacing="2" cellpadding="2" border="0">
        <td colspan="3" rowspan="1" style="vertical-align: top;">
                  <span style="font-family: Arial;  font-size: 22px">
                      <a style="text-decoration:underline;" href="https://1429904852.github.io">Home</a>
                  </span>
        </td>
    </table>
</div>
<br/>




(* indicates equal contribution)

<HR style="border:1 solid #cecccc" width="100%" SIZE=1>
<!-- <h3><font size=4.5 face="Arial" color="blue"><font color=black>2020</font></font></h3> -->
<ul>
    <td>
        <b>Learning from Different text-image Pairs: A Relation-enhanced Graph Convolutional Network for Multimodal NER</b>
        <a href=""><font color=""><font color=blue>[Paper]</font></font></a>
        <a href="https://github.com/1429904852/R-GCN"><font color=""><font color=blue>[Code]</font></font></a>
        <a href="https://mp.weixin.qq.com/s/cDAXxLrDIVjWofLFaam_KA"><font color=""><font color=blue>[中文解读]</font></font></a>
        <a href=""><font color=""><font color=blue>[BibTex]</font></font></a>
        <br/><U><I>Fei Zhao</I></U>, Chunhui li, Zhen Wu, Shangyu Xing, Xinyu Dai
        <br/> Summary: we propose leveraging the external matching relations between different (text, image) pairs to improve the performance on the MNER task.
        <br/>ACMMM, 2022.
        <br/>会议等级: CCF A
        <br/><br/>

        <b>Attention Transfer Network for Aspect-level Sentiment Classification</b>
        <a href="https://aclanthology.org/2020.coling-main.70.pdf"><font color=""><font color=blue>[Paper]</font></font></a>
        <a href="https://github.com/1429904852/ATN"><font color=""><font color=blue>[Code]</font></font></a>
        <a href="https://mp.weixin.qq.com/s/e59MHQ5wqH1HN8WrkTaA4Q"><font color=""><font color=blue>[中文解读]</font></font></a>
        <a href="../bibtex/COLING2020_ATN.txt"><font color=""><font color=blue>[BibTex]</font></font></a>
        <br/><U><I>Fei Zhao*</I></U>, Zhen Wu*, Xin-Yu Dai
        <br/> Summary: Transter opinion knowledge from resource-rich document-level data to improve aspect-level sentiment classification.
        <br/><font color="red">(Oral)</font> COLING, 2020.
        <br/>会议等级: CCF B
        <br/><br/>

        <b>Grid Tagging Scheme for Aspect-oriented Fine-grained Opinion Extraction</b>
        <a href="https://aclanthology.org/2020.findings-emnlp.234.pdf"><font color=""><font color=blue>[Paper]</font></font></a>
        <a href="https://github.com/NJUNLP/GTS"><font color=""><font color=blue>[Datasets and Code]</font></font></a><img
        src="https://img.shields.io/github/stars/NJUNLP/GTS?style=social"/>
        <a href="https://mp.weixin.qq.com/s/cOCmao-hWdauRETJgSNbJQ"><font color=""><font color=blue>[中文解读]</font></font></a>
        <a href="../bibtex/EMNLP2020_Findings_GTS.txt"><font color=""><font color=blue>[BibTex]</font></font></a>
        <br/>Zhen Wu, Chengcan Ying, <U><I>Fei Zhao</I></U>, Zhifang Fan, Xinyu Dai, Rui Xia
        <br/> Summary: A poweful and unified tagging scheme to address fine-grained opinion extraction in the form of opinion pair or opinion triplet.
        <br/>EMNLP Findings, 2020.
        <br/><br/>
        
        <b>Latent Opinions Transfer Network for Target-Oriented Opinion Words Extraction</b>
        <a href="https://arxiv.org/pdf/2001.01989.pdf"><font color=""><font color=blue>[Paper]</font></font></a>
        <a href="https://github.com/1429904852/LOTN"><font color=""><font color=blue>[Code]</font></font></a>
        <a href="https://mp.weixin.qq.com/s/rO_dGBt_x6uIXLsjka5kKA"><font color=""><font color=blue>[中文解读]</font></font></a>
        <a href="../bibtex/AAAI2020_LOTN.txt"><font color=""><font color=blue>[BibTex]</font></font></a>
        <br/>Zhen Wu*, <U><I>Fei Zhao*</I></U>, Xin-Yu Dai, Shujian Huang, Jiajun Chen
        <br/> Summary: Transter opinion knowledge from resource-rich document-level data to improve target-oriented opinion words extraction.
        <br/><font color="red">(Oral)</font> AAAI, 2020.
        <br/>会议等级: CCF A
        <br/><br/>

    </td>
</ul>

</span>
</div>

</body>

</html>

